{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOWJwKylVBWd7UJMJQM5NqE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"o1GKAZt7AQgG"},"outputs":[],"source":["import numpy as np\n","import torch\n","import tensorflow as tf\n","import time\n","from Bio import SeqIO\n","from torch_geometric.data import Data\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, cohen_kappa_score, matthews_corrcoef, confusion_matrix\n","from transformers import AutoModel, AutoTokenizer\n","\n","# Load pre-trained\n","MODEL_NAME = \"facebook/esm2_t6_8M_UR50D\"  # Example: ESM model\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","embedding_model = AutoModel.from_pretrained(MODEL_NAME)\n","\n","\n","def load_fasta(file_path, label):\n","    sequences, labels = [], []\n","    for record in SeqIO.parse(file_path, \"fasta\"):\n","        sequences.append(str(record.seq))\n","        labels.append(label)\n","    return sequences, labels\n","\n","\n","def get_protein_embedding(sequence):\n","    tokens = tokenizer(sequence, return_tensors=\"pt\", padding=True, truncation=True)\n","    with torch.no_grad():\n","        embeddings = embedding_model(**tokens).last_hidden_state.mean(dim=1).squeeze()\n","    return embeddings.numpy()\n","\n","\n","# Load data\n","train_pos_file = \"train_positive.fasta\"\n","train_neg_file = \"train_negative.fasta\"\n","test_pos_file = \"test_positive.fasta\"\n","test_neg_file = \"test_negative.fasta\"\n","\n","train_pos_sequences, train_pos_labels = load_fasta(train_pos_file, 1)\n","train_neg_sequences, train_neg_labels = load_fasta(train_neg_file, 0)\n","test_pos_sequences, test_pos_labels = load_fasta(test_pos_file, 1)\n","test_neg_sequences, test_neg_labels = load_fasta(test_neg_file, 0)\n","\n","# Combine and encode sequences\n","train_sequences = train_pos_sequences + train_neg_sequences\n","test_sequences = test_pos_sequences + test_neg_sequences\n","train_labels = np.array(train_pos_labels + train_neg_labels)\n","test_labels = np.array(test_pos_labels + test_neg_labels)\n","\n","train_embeddings = np.array([get_protein_embedding(seq) for seq in train_sequences])\n","test_embeddings = np.array([get_protein_embedding(seq) for seq in test_sequences])\n","\n","# Train-validation split\n","X_train, X_val, y_train, y_val = train_test_split(train_embeddings, train_labels, test_size=0.2, random_state=42)\n","X_test, y_test = test_embeddings, test_labels\n","\n","# ---------------------- BiLSTM with Attention ---------------------- #\n","def build_bilstm_attention_model(input_shape):\n","    inputs = tf.keras.Input(shape=input_shape)\n","    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(inputs)\n","    attention = tf.keras.layers.Attention()([x, x])\n","    x = tf.keras.layers.GlobalAveragePooling1D()(attention)\n","    x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n","    x = tf.keras.layers.Dropout(0.4)(x)\n","    outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n","    return tf.keras.Model(inputs, outputs)\n","\n","# ---------------------- CNN-BiLSTM Hybrid ---------------------- #\n","def build_cnn_bilstm_model(input_shape):\n","    inputs = tf.keras.Input(shape=input_shape)\n","    x = tf.keras.layers.Conv1D(64, kernel_size=3, activation=\"relu\", padding=\"same\")(inputs)\n","    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n","    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(x)\n","    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n","    x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n","    x = tf.keras.layers.Dropout(0.4)(x)\n","    outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n","    return tf.keras.Model(inputs, outputs)\n","\n","# ---------------------- Transformer-based Model ---------------------- #\n","def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.4):\n","    x = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=head_size, dropout=dropout)(inputs, inputs)\n","    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x + inputs)\n","    x = tf.keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n","    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n","    return x\n","\n","\n","def build_transformer_model(input_shape):\n","    inputs = tf.keras.Input(shape=input_shape)\n","    x = transformer_encoder(inputs, head_size=256, num_heads=4, ff_dim=4)\n","    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n","    x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n","    x = tf.keras.layers.Dropout(0.4)(x)\n","    outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n","    return tf.keras.Model(inputs, outputs)\n","\n","# ---------------------- Model Selection & Training ---------------------- #\n","input_shape = (train_embeddings.shape[1], 1)\n","\n","models = {\n","    \"BiLSTM_Attention\": build_bilstm_attention_model(input_shape),\n","    \"CNN_BiLSTM\": build_cnn_bilstm_model(input_shape),\n","    \"Transformer\": build_transformer_model(input_shape)\n","}\n","\n","for model_name, model in models.items():\n","    print(f\"Training {model_name}...\")\n","    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n","    start_time = time.time()\n","    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=1)\n","    training_time = time.time() - start_time\n","\n","    y_pred_probs = model.predict(X_test)\n","    y_pred = (y_pred_probs > 0.5).astype(int)\n","\n","    accuracy = accuracy_score(y_test, y_pred)\n","    precision = precision_score(y_test, y_pred)\n","    recall = recall_score(y_test, y_pred)\n","    f1 = f1_score(y_test, y_pred)\n","    auc = roc_auc_score(y_test, y_pred_probs)\n","    kappa = cohen_kappa_score(y_test, y_pred)\n","    mcc = matthews_corrcoef(y_test, y_pred)\n","    conf_matrix = confusion_matrix(y_test, y_pred)\n","\n","    print(f\"{model_name} Results:\")\n","    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}, Kappa: {kappa:.4f}, MCC: {mcc:.4f}\")\n","    print(f\"Confusion Matrix:\\n{conf_matrix}\")\n","    print(f\"Execution Time: {training_time:.2f} seconds\\n\")\n"]}]}